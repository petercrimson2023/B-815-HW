<<<<<<< HEAD
---
title: "Report_Of_Question"
author: "Bulun Te"
date: "2024-01-24"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


Actually my most important question now is the gradient descent either with constant step size or with BB adpated step size could not converge to true value even in a 2 component mixture problem.

The data generating process is like this:

```{r}

library(dplyr)
source("functions.R")

data_points = default_data_generation(n=500)
```

first using the mclust package trying to fit the data

```{r}

library(mclust)
mclust_result = Mclust(data_points)

print(mclust_result$parameters)
```


Then, to conduct descent algorithm, my Loss function and derivative is defined as follow:

I applied exponential transfomation to adjust for the requirement of the constraint that the sum of the weights should be 1.

$\pi_k = \frac{exp(\gamma_k)}{\sum_{k=1}^m exp(\gamma_k)}$

And then the loss function becomes like this $\phi$ is the density function of normal distribution: 

$L(x|\mu,\sigma,\gamma) = -\sum ( \log ( [ \sum_{i=1}^{m} \frac{e^{\gamma_i}}{\sum_{j=1}^{k} e^{\gamma_j}}  \phi(x | \mu_i , \sigma_i ) ]))$

After constructing the loss function I first tried to use the optim function in R to get the result. I used the BFGS method and the result is like this:

Although, program gives a lot of warnings, the result is close to the true points

```{r}

x = c(5,4,0,0,2,1) # randomly chosen starting point
optim_result = optim(x, loss_function,data_points=data_points, method = "BFGS")
parameter = optim_result$par
parameter[1:2] = exp(parameter[1:2]) / sum(exp(parameter[1:2])) #calculating pi
parameter

```
After that I tried to implement gradient descent algorithm with constant step size and BB adapted step size.

The deravative of the loss function is like this:

$\frac{\partial L}{\partial \gamma_i} = - \sum_{j=1}^n [ \frac{\phi (x_j | \mu_i, \sigma_i ) exp(\gamma_i)}{\sum_{i=1}^m \phi (x_j | \mu_i, \sigma_i ) exp(\gamma_i)}] - n\frac{exp(\gamma_i)}{\sum_{i=1}^m exp(\gamma_i)}$

$\frac{\partial L}{\partial \mu_i} = - \sum_{j=1}^n [ \frac{\phi (x_j | \mu_i, \sigma_i ) exp(\gamma_i)}{\sum_{i=1}^m \phi (x_j | \mu_i, \sigma_i ) exp(\gamma_i)}] \frac{x_j - \mu_i}{\sigma_i^2} $

$\frac{\partial L}{\partial \sigma_i} = - \sum_{j=1}^n [ \frac{\phi (x_j | \mu_i, \sigma_i ) exp(\gamma_i)}{\sum_{i=1}^m \phi (x_j | \mu_i, \sigma_i ) exp(\gamma_i)} \times ( \frac{(x_j - \mu_i)^2}{\sigma_i^3} - \frac{1}{\sigma_i})]$


Checking the derivative function:

```{r}

x = c(1,2,1,-1,0.33,0.2) # randomly chosen starting point

grad_function(x,data_points) %>% as.vector()

numerical_grad_list = c()

for(i in 1:length(x)){
  x_epsilon = x
  x_m_epsilon = x
  x_epsilon[i] = x[i] + 1e-6
  x_m_epsilon[i] = x[i] - 1e-6
  value = ((loss_function(x_epsilon,data_points) - loss_function(x_m_epsilon,data_points))/(2*1e-6)) %>% as.vector()
  numerical_grad_list = c(numerical_grad_list,value)
}

numerical_grad_list



```

Gradients calculated by function are nearly same as the numerical gradients.

The gradient descent algorithm with constant step size is implemented as follow:

There is some different on the reuslt of one of the $\mu$, and other results seems to be close to the true point.

```{r}

x = c(1,2,1,-1,0.33,0.2) 
# near to the true point, gives the result near to true value, iteration 65

#x = c(5,4,0,0,2,1)
# another initial points which gives nearly true result
# but takes more time to converge iteration 433

result_list = gradient_descent(x,data_points,step_size = 0.0001,max_iter = 5000,report =FALSE)
result_list[2:5]

result_list$loss[2:result_list$iter] %>% plot( ,type="l",main="loss")
result_list$u_frame%>% plot(,main="u")
result_list$sigma_frame%>% plot(,main="sigma")
result_list$pi_frame%>% plot(,main="pi",ylim=c(0,1))

#plotting_reuslt(result_list)

```

Then I tried to implement the gradient descent algorithm with BB adapted step size, the result is like this:

```{r

```

Then a constant stepsize is changed to a descending stepsize by $stepsize_{t+1}=\frac{1}{1+0.1t}\times stepsize_t$ and the result is as follow:

```{r}

#x = c(5,4,0,0,2,1) 
# converged at iteration 515 but with sightly different pi value: 0.3909 0.6091
# where true value shoule be 1/3, 2/3

x = c(1,2,1,-1,0.33,0.2) 
# near the true point, converged at iteration 80
# accuracy becomes more higher than the constant stepsize

result_list = gradient_dec_descent(x,data_points,step_size = 0.001,max_iter = 1000,report =FALSE)
result_list[2:5]

result_list$loss[2:result_list$iter] %>% plot( ,type="l",main="loss")
result_list$grad %>% plot( ,type="l",main="grad")

result_list$u_frame%>% plot(,main="u")
result_list$sigma_frame%>% plot(,main="sigma")
result_list$pi_frame%>% plot(,main="pi",ylim=c(0,1))

```

Applying BB method to the same data set




```{r}

x = c(1,2,1,-1,0.33,0.2) 
#near to true value, converged to true value within 41 steps

#x = c(5,4,0,0,2,1) # not converged to true value
#x=c(1,3,0,0,1,1) # not converged to true value
#x=c(1,2,0,0,1,1) # not converged to true value

#x=c(1.1,1.9,0.9,-0.8,1/2,1/3) # not converged to true value

#x=c(1,1,1,1,1,1) #not congerged 

result_list = gradient_BB_descent(x,data_points,max_iter =1000,tol = 1e-5, loss_tol=1e-6,report = FALSE)
result_list[2:5]

result_list$loss[5:result_list$iter] %>% plot( ,type="l",main="loss")
result_list$grad %>% plot( ,type="l",main="absolute norm of grad")

result_list$u_frame%>% plot(,main="u")
result_list$sigma_frame%>% plot(,main="sigma")
result_list$pi_frame%>% plot(,main="pi",ylim=c(0,1))


```


Applying Nestrov Accleration Method to same dataset
letting sigma reparameterized as $|\sigma|$ and using subgradients

$\frac{\partial L}{\partial \gamma_i} = - \sum_{j=1}^n [ \frac{\phi (x_j | \mu_i, |\sigma_i| ) exp(\gamma_i)}{\sum_{i=1}^m \phi (x_j | \mu_i, |\sigma_i| ) exp(\gamma_i)}] - n\frac{exp(\gamma_i)}{\sum_{i=1}^m exp(\gamma_i)}$

$\frac{\partial L}{\partial \mu_i} = - \sum_{j=1}^n [ \frac{\phi (x_j | \mu_i,| \sigma_i |) exp(\gamma_i)}{\sum_{i=1}^m \phi (x_j | \mu_i, |\sigma_i| ) exp(\gamma_i)}] \frac{x_j - \mu_i}{\sigma_i^2} $

$\frac{\partial L}{\partial \sigma_i} = - \sum_{j=1}^n [ \frac{\phi (x_j | \mu_i, \sigma_i ) exp(\gamma_i)}{\sum_{i=1}^m \phi (x_j | \mu_i, \sigma_i ) exp(\gamma_i)} \times ( \frac{(x_j - \mu_i)^2 \times sign(\sigma) }{\sigma_i^3} - \frac{sign(\sigma)}{\sigma_i})]$


```{r}

x = c(1,2,1,-5,sqrt(0.1),1) 

#x = c(1,1+log(2),1,-5,1/3,1/5 )#true point

#x = c(5,4,0,0,0.5,0.2) # another initial points, converged to local optimum. step_zie = 1e-4

result_list = gradient_Nestrov_descent(x,data_points,step_size = 1e-2/5,max_iter = 2000,report =FALSE)


result_list[2:5]
result_list$loss[2:result_list$iter] %>% plot( ,type="l",main="loss")
result_list$grad %>% plot( ,type="l",main="absolute norm of grad")
result_list$u_frame%>% plot(,main="u")
result_list$sigma_frame%>% plot(,main="sigma")
result_list$pi_frame%>% plot(,main="pi",ylim=c(0,1))


```


