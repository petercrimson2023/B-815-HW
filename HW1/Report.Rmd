---
title: "Report_Of_Question"
author: "Bulun Te"
date: "2024-01-24"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


Actually my most important question now is the gradient descent either with constant step size or with BB adpated step size could not converge to true value even in a 2 component mixture problem.

The data generating process is like this:

```{r}

library(dplyr)
set.seed(123)

u1 = 1
sigma1 = 1/3

u2=-1
sigma2=1/5

pi_1 = 1/3
pi_2 = 2/3

n=100
uniform_random = runif(n,0,1)
data_points = (uniform_random<=pi_1) * rnorm(n, u1, sigma1) + (uniform_random>pi_1) * rnorm(n, u2, sigma2)


```
I first used the mclust package trying to fit the data and the result is like this:

```{r}

library(mclust)
mclust_result = Mclust(data_points)
mclust_result$parameters

```


Then, to conduct descent algorithm, my Loss function and derivative is defined as follow:

I applied exponential transfomation to adjust for the requirement of the constraint that the sum of the weights should be 1.

$\pi_k = \frac{exp(\gamma_k)}{\sum_{k=1}^m exp(\gamma_k)}$

And then the loss function becomes like this $\phi$ is the density function of normal distribution: 

$L(x|\mu,\sigma,\gamma) = -\sum ( \log ( [ \sum_{i=1}^{m} \frac{e^{\gamma_i}}{\sum_{j=1}^{k} e^{\gamma_j}}  \phi(x | \mu_i , \sigma_i ) ]))$


```{r}

loss_function = function(x, data_points = data_points) {
  n = length(data_points)
  k = length(x) / 3
  loss = 0
  
  pi_value = exp(x[1:k]) / sum(exp(x[1:k]))
  
  density_list = list()
  for (i in 1:k) {
    density_list[[i]] = dnorm(data_points, x[k + i], x[2 * k + i])
  }
  density_matrix = data.frame(density_list) %>% as.matrix()
  
  loss = (density_matrix %*% pi_value) + 1e-5   # I added 1e-5 to avoid the log(0) problem
  
  loss = loss %>% log() %>% sum()
  
  return(-loss)
  
  
}

```


After constructing the loss function I first tried to use the optim function in R to get the result. I used the BFGS method and the result is like this:

Although, program gives a lot of warnings, the result is close to the true points

```{r}

x = c(5,4,0,0,2,1)
optim_result = optim(x, loss_function,data_points=data_points, method = "BFGS")
parameter = optim_result$par

parameter[1:2] = exp(parameter[1:2]) / sum(exp(parameter[1:2])) #calculating pi

parameter

```
After that I tried to implement gradient descent algorithm with constant step size and BB adapted step size.

The deravative of the loss function is like this:

$\frac{\partial L}{\partial \gamma_i} = - \sum_{j=1}^n [ \frac{\phi (x_j | \mu_i, \sigma_i ) exp(\gamma_i)}{\sum_{i=1}^m \phi (x_j | \mu_i, \sigma_i ) exp(\gamma_i)}] - n\frac{exp(\gamma_i)}{\sum_{i=1}^m exp(\gamma_i)}$

$\frac{\partial L}{\partial \mu_i} = - \sum_{j=1}^n [ \frac{\phi (x_j | \mu_i, \sigma_i ) exp(\gamma_i)}{\sum_{i=1}^m \phi (x_j | \mu_i, \sigma_i ) exp(\gamma_i)}] \frac{x_j - \mu_i}{\sigma_i^2} $

$\frac{\partial L}{\partial \sigma_i} = - \sum_{j=1}^n [ \frac{\phi (x_j | \mu_i, \sigma_i ) exp(\gamma_i)}{\sum_{i=1}^m \phi (x_j | \mu_i, \sigma_i ) exp(\gamma_i)} \times ( \frac{(x_j - \mu_i)^2}{\sigma_i^3} - \frac{1}{\sigma_i})]$

And the gradient function is implemented as follow:

```{r}

grad_function = function(x, data_points) {
  # Initialize the number of data points and the number of components in the mixture model
  n = length(data_points)
  k = length(x) / 3
  # Calculate the weights of each Gaussian component in the mixture
  pi_value = exp(x[1:k]) / sum(exp(x[1:k]))
  # Initialize lists to store intermediate calculations for each component
  exp_gamma_density_list = list()
  centered_data_point_list = list()
  second_order_centered_data_point_list = list()
  sum_exp_gamma_density = rep(0, n)
  
  for (i in 1:k) {
    # Calculate the density of each data point under the current Gaussian component
    density = dnorm(data_points, x[k + i], x[2 * k + i])
    exp_gamma_density_value = density * exp(x[i])
    
    exp_gamma_density_list[[i]] = exp_gamma_density_value
    sum_exp_gamma_density = sum_exp_gamma_density + exp_gamma_density_value + 1e-5
    
    # Calculate the centered data points and their second order for the current component
    centered_data_point_list[[i]] = (data_points - x[k + i]) / (x[2 * k + i])
    second_order_centered_data_point_list[[i]] = (data_points - x[k + i]) ^ 2 / (x[2 * k + i] ^ 3) - 1 / (x[2 * k + i])
  }

  # Convert the lists to matrices for vectorized operations  
  exp_gamma_density_matrix = data.frame(exp_gamma_density_list) %>% as.matrix()
  exp_gamma_density_matrix = exp_gamma_density_matrix / sum_exp_gamma_density
  
  centered_data_point_matrix = data.frame(centered_data_point_list) %>% as.matrix()
  second_order_centered_data_point_matrix = data.frame(second_order_centered_data_point_list) %>% as.matrix()
  
  grad_gamma = -(colSums(exp_gamma_density_matrix) - pi_value * n)
  grad_u = -(centered_data_point_matrix * exp_gamma_density_matrix) %>% colSums()
  grad_sigma = -(second_order_centered_data_point_matrix * exp_gamma_density_matrix) %>% colSums()
  
  # Return the concatenated gradient vector  
  return(c(grad_gamma, grad_u, grad_sigma))
}


```


The gradient descent algorithm with constant step size is implemented as follow:

```{r}

gradient_descent = function(x,
                            data_points,
                            step_size = 0.01,
                            max_iter = 1000,
                            tol = 1e-3,
                            report = TRUE) {
  iter = 0
  x_old = rep(0, length(x))
  k = length(x) / 3
  loss_list = c()
  u_frame = data.frame()
  sigma_frame = data.frame()
  pi_frame = data.frame()
  
  loss = loss_function(x, data_points)
  
  if (is.nan(loss) || is.infinite(loss)) {
    stop("Initial loss is NaN or infinite")
  }
  
  u_frame = rbind(x[(k + 1):(2 * k)], u_frame)
  sigma_frame = rbind(x[(2 * k + 1):(3 * k)], sigma_frame)
  pi_frame = rbind(exp(x[1:k]) / sum(exp(x[1:k])), pi_frame)
  loss_list = c(loss_list, loss)
  grad_list = c()
  
  while (iter < max_iter) {
    iter = iter + 1
    x_old = x
    
    gradient = grad_function(x, data_points)
  
    if (any(is.nan(gradient)) || any(is.infinite(gradient))) {
      
      print(paste0("Gradient is NaN or infinite at iteration ", iter))
      names(pi_frame) = paste0("pi_", 1:k)
      names(u_frame) = paste0("u_", 1:k)
      names(sigma_frame) = paste0("sigma_", 1:k)
      
      result_list = list(
        loss = loss_list,
        pi_value = (exp(x[1:k]) / sum(exp(x[1:k]))) %>% as.vector() ,
        u = x[(k + 1):(2 * k)] %>% as.vector(),
        sigma = x[(2 * k + 1):(3 * k)] %>% as.vector(),
        iter = iter,
        grad = grad_list,
        u_frame = u_frame,
        sigma_frame = sigma_frame,
        pi_frame = pi_frame
      )
      
      
      
      return(result_list)
      
      
    }
    #Determining the stepsize
    #x = x - step_size * (1 / (1 + 0.1 * iter)) * grad
    
    x = x - step_size * gradient
    
    u_frame=rbind(x[(k+1):(2*k)],u_frame)
    sigma_frame=rbind(x[(2*k+1):(3*k)],sigma_frame)
    pi_frame=rbind(exp(x[1:k])/sum(exp(x[1:k])),pi_frame)
    
    loss_new = loss_function(x, data_points)
    
    if (is.nan(loss_new) || is.infinite(loss_new)) {
      names(pi_frame) = paste0("pi_", 1:k)
      names(u_frame) = paste0("u_", 1:k)
      names(sigma_frame) = paste0("sigma_", 1:k)
      print(paste0("Loss is NaN or infinite at iteration ", iter))
      result_list = list(
        loss = loss_list,
        pi_value = (exp(x[1:k]) / sum(exp(x[1:k]))) %>% as.vector() ,
        u = x[(k + 1):(2 * k)] %>% as.vector(),
        sigma = x[(2 * k + 1):(3 * k)] %>% as.vector(),
        iter = iter,
        grad = grad_list,
        u_frame = u_frame,
        sigma_frame = sigma_frame,
        pi_frame = pi_frame
      )
      
      
      return(result_list)
    }
    
    loss_list = c(loss_list, loss_new)
    grad_list = c(grad_list, max(abs(gradient)))
    
    if (report) {
      cat("iter:", iter, "loss:", loss_new, "\n")
      cat("pi:", exp(x[1:k]) / sum(exp(x[1:k])), "\n")
      cat("u:", x[(k + 1):2 * k] , "\n")
      cat("sigma:", x[(2 * k + 1):(3 * k)] , "\n")
    }
    
    if (max(abs(x - x_old)) < tol) {
      break
    }
  }
  
  names(pi_frame) = paste0("pi_", 1:k)
  names(u_frame) = paste0("u_", 1:k)
  names(sigma_frame) = paste0("sigma_", 1:k)
  
  result_list = list(
    loss = loss_list,
    pi_value = (exp(x[1:k]) / sum(exp(x[1:k]))) %>% as.vector() ,
    u = x[(k + 1):(2 * k)] %>% as.vector(),
    sigma = x[(2 * k + 1):(3 * k)] %>% as.vector(),
    iter = iter,
    grad = grad_list,
    u_frame = u_frame,
    sigma_frame = sigma_frame,
    pi_frame = pi_frame
  )
  
  return(result_list)
}
```

The result of simulation fron the true point is as follow:

There is some different on the reuslt of one of the $\mu$, and other results seems to be close to the true point.

```{r}
#x = c(1,2,1,-1,0.33,0.2) #true point

x = c(5,4,0,0,2,1) # another initial points which gives similar result but takes more times to converge

result_list = gradient_descent(x,data_points,step_size = 0.001,max_iter = 5000,report =FALSE)
result_list[2:5]

result_list$loss[2:result_list$iter] %>% plot( ,type="l",main="loss")
result_list$grad %>% plot( ,type="l",main="absolute norm of grad")

result_list$u_frame%>% plot(,main="u")
result_list$sigma_frame%>% plot(,main="sigma")
result_list$pi_frame%>% plot(,main="pi",ylim=c(0,1))


```


Then I changed the constant stepsize to a descending stepsize by $stepsize_{t+1}=\frac{1}{1+0.1t}\times stepsize_t$ and the result is as follow:

```{r,echo=FALSE}

gradient_dec_descent = function(x,
                            data_points,
                            step_size = 0.01,
                            max_iter = 1000,
                            tol = 1e-3,
                            report = TRUE) {
  iter = 0
  x_old = rep(0, length(x))
  k = length(x) / 3
  loss_list = c()
  u_frame = data.frame()
  sigma_frame = data.frame()
  pi_frame = data.frame()
  
  loss = loss_function(x, data_points)
  
  if (is.nan(loss) || is.infinite(loss)) {
    stop("Initial loss is NaN or infinite")
  }
  
  u_frame = rbind(x[(k + 1):(2 * k)], u_frame)
  sigma_frame = rbind(x[(2 * k + 1):(3 * k)], sigma_frame)
  pi_frame = rbind(exp(x[1:k]) / sum(exp(x[1:k])), pi_frame)
  loss_list = c(loss_list, loss)
  grad_list = c()
  
  while (iter < max_iter) {
    iter = iter + 1
    x_old = x
    
    gradient = grad_function(x, data_points)
  
    if (any(is.nan(gradient)) || any(is.infinite(gradient))) {
      
      print(paste0("Gradient is NaN or infinite at iteration ", iter))
      names(pi_frame) = paste0("pi_", 1:k)
      names(u_frame) = paste0("u_", 1:k)
      names(sigma_frame) = paste0("sigma_", 1:k)
      
      result_list = list(
        loss = loss_list,
        pi_value = (exp(x[1:k]) / sum(exp(x[1:k]))) %>% as.vector() ,
        u = x[(k + 1):(2 * k)] %>% as.vector(),
        sigma = x[(2 * k + 1):(3 * k)] %>% as.vector(),
        iter = iter,
        grad = grad_list,
        u_frame = u_frame,
        sigma_frame = sigma_frame,
        pi_frame = pi_frame
      )
      return(result_list)
    }
    #Determining the stepsize
    x = x - step_size * (1 / (1 + 0.1 * iter)) * gradient
    #x = x - step_size * gradient
    
    u_frame=rbind(x[(k+1):(2*k)],u_frame)
    sigma_frame=rbind(x[(2*k+1):(3*k)],sigma_frame)
    pi_frame=rbind(exp(x[1:k])/sum(exp(x[1:k])),pi_frame)
    
    loss_new = loss_function(x, data_points)
    
    if (is.nan(loss_new) || is.infinite(loss_new)) {
      names(pi_frame) = paste0("pi_", 1:k)
      names(u_frame) = paste0("u_", 1:k)
      names(sigma_frame) = paste0("sigma_", 1:k)
      print(paste0("Loss is NaN or infinite at iteration ", iter))
      result_list = list(
        loss = loss_list,
        pi_value = (exp(x[1:k]) / sum(exp(x[1:k]))) %>% as.vector() ,
        u = x[(k + 1):(2 * k)] %>% as.vector(),
        sigma = x[(2 * k + 1):(3 * k)] %>% as.vector(),
        iter = iter,
        grad = grad_list,
        u_frame = u_frame,
        sigma_frame = sigma_frame,
        pi_frame = pi_frame
      )
      return(result_list)
    }
    
    loss_list = c(loss_list, loss_new)
    grad_list = c(grad_list, max(abs(gradient)))
    
    if (report) {
      cat("iter:", iter, "loss:", loss_new, "\n")
      cat("pi:", exp(x[1:k]) / sum(exp(x[1:k])), "\n")
      cat("u:", x[(k + 1):2 * k] , "\n")
      cat("sigma:", x[(2 * k + 1):(3 * k)] , "\n")
    }
    
    if (max(abs(x - x_old)) < tol) {
      break
    }
  }
  
  names(pi_frame) = paste0("pi_", 1:k)
  names(u_frame) = paste0("u_", 1:k)
  names(sigma_frame) = paste0("sigma_", 1:k)
  
  result_list = list(
    loss = loss_list,
    pi_value = (exp(x[1:k]) / sum(exp(x[1:k]))) %>% as.vector() ,
    u = x[(k + 1):(2 * k)] %>% as.vector(),
    sigma = x[(2 * k + 1):(3 * k)] %>% as.vector(),
    iter = iter,
    grad = grad_list,
    u_frame = u_frame,
    sigma_frame = sigma_frame,
    pi_frame = pi_frame
  )
  
  return(result_list)
}

```


```{r}

x = c(5,4,0,0,2,1) 
#x = c(1,2,1,-1,0.33,0.2) #true point
result_list = gradient_dec_descent(x,data_points,step_size = 0.01,max_iter = 1000,report =FALSE)
result_list[2:5]

result_list$loss[2:result_list$iter] %>% plot( ,type="l",main="loss")
result_list$grad %>% plot( ,type="l",main="grad")

result_list$u_frame%>% plot(,main="u")
result_list$sigma_frame%>% plot(,main="sigma")
result_list$pi_frame%>% plot(,main="pi",ylim=c(0,1))


```



