result_list = list(
loss = loss_list,
pi_value = (exp(x[1:k]) / sum(exp(x[1:k]))) %>% as.vector() ,
u = x[(k + 1):(2 * k)] %>% as.vector(),
sigma = x[(2 * k + 1):(3 * k)] %>% as.vector(),
iter = iter,
grad = grad_list,
u_frame = u_frame,
sigma_frame = sigma_frame,
pi_frame = pi_frame
)
print(paste0("Loss is NaN or infinite at iteration ", iter))
return(result_list)
}
loss_list = c(loss_list, loss_new)
grad_list = c(grad_list, max(abs(gradient)))
if (report) {
cat("iter:", iter, "loss:", loss_new, "\n")
cat("pi:", exp(x[1:k]) / sum(exp(x[1:k])), "\n")
cat("u:", x[(k + 1):(2 * k)] , "\n")
cat("sigma:", x[(2 * k + 1):(3 * k)] , "\n")
}
# if (max(abs(x-x_old)) < tol || abs(loss_new - loss_old) < loss_tol) {
#   break
# }
if (max(abs(x-x_old)) < tol ) {
break
}
gradient_old = gradient
x_old_2 = x_old
}
names(pi_frame) = paste0("pi_",1:k)
names(u_frame) = paste0("u_",1:k)
names(sigma_frame) = paste0("sigma_",1:k)
result_list = list(
loss = loss_list,
pi_value = (exp(x[1:k]) / sum(exp(x[1:k]))) %>% as.vector() ,
u = x[(k + 1):(2 * k)] %>% as.vector(),
sigma = x[(2 * k + 1):(3 * k)] %>% as.vector(),
iter = iter,
grad = grad_list,
u_frame = u_frame,
sigma_frame = sigma_frame,
pi_frame = pi_frame
)
return(result_list)
}
gradient_BB_descent = function(x,
data_points,
max_iter = 1000,
tol = 1e-5,
loss_tol=1e-5,
report = FALSE) {
iter = 0
x_old = rep(0, length(x))
k = length(x) / 3
loss_list = c()
grad_list = c()
u_frame = data.frame()
sigma_frame = data.frame()
pi_frame = data.frame()
loss_new = loss_function(x, data_points)
if (is.nan(loss) || is.infinite(loss)) {
stop("Initial loss is NaN or infinite")
}
loss_list = c(loss_list, loss)
gradient_old = rep(0, length(x))
x_old_2 = rep(0, length(x))
u_frame=rbind(x[(k+1):(2*k)],u_frame)
sigma_frame=rbind(x[(2*k+1):(3*k)],sigma_frame)
pi_frame=rbind(exp(x[1:k])/sum(exp(x[1:k])),pi_frame)
while (iter < max_iter) {
iter = iter + 1
x_old = x
gradient = grad_function(x, data_points)
if (any(is.nan(gradient)) || any(is.infinite(gradient))) {
print(as.vector(x_old))
names(pi_frame) = paste0("pi_",1:k)
names(u_frame) = paste0("u_",1:k)
names(sigma_frame) = paste0("sigma_",1:k)
result_list = list(
loss = loss_list,
pi_value = (exp(x[1:k]) / sum(exp(x[1:k]))) %>% as.vector() ,
u = x[(k + 1):2 * k] %>% as.vector(),
sigma = x[(2 * k + 1):(3 * k)] %>% as.vector(),
iter = iter,
grad = grad_list,
u_frame = u_frame,
sigma_frame = sigma_frame,
pi_frame = pi_frame
)
print(paste0("Gradient is NaN or infinite at iteration ", iter))
return(result_list)
#stop("Gradient is NaN or infinite at iteration ", iter)
#print(x_old)
}
step_size = as.vector((x_old - x_old_2) %*% (gradient - gradient_old) ) / sum((gradient - gradient_old) ^ 2)
#step_size = step_size * max(1,log10(1 + iter))
x = x - step_size * gradient
u_frame=rbind(x[(k+1):(2*k)],u_frame)
sigma_frame=rbind(x[(2*k+1):(3*k)],sigma_frame)
pi_frame=rbind(exp(x[1:k])/sum(exp(x[1:k])),pi_frame)
x[(2*k+1):(3*k)]=abs(x[(2*k+1):(3*k)])
loss_old = loss_new
loss_new = loss_function(x, data_points)
if (is.nan(loss_new) || is.infinite(loss_new)) {
names(pi_frame) = paste0("pi_",1:k)
names(u_frame) = paste0("u_",1:k)
names(sigma_frame) = paste0("sigma_",1:k)
result_list = list(
loss = loss_list,
pi_value = (exp(x[1:k]) / sum(exp(x[1:k]))) %>% as.vector() ,
u = x[(k + 1):(2 * k)] %>% as.vector(),
sigma = x[(2 * k + 1):(3 * k)] %>% as.vector(),
iter = iter,
grad = grad_list,
u_frame = u_frame,
sigma_frame = sigma_frame,
pi_frame = pi_frame
)
print(paste0("Loss is NaN or infinite at iteration ", iter))
return(result_list)
}
loss_list = c(loss_list, loss_new)
grad_list = c(grad_list, max(abs(gradient)))
if (report) {
cat("iter:", iter, "loss:", loss_new, "\n")
cat("pi:", exp(x[1:k]) / sum(exp(x[1:k])), "\n")
cat("u:", x[(k + 1):(2 * k)] , "\n")
cat("sigma:", x[(2 * k + 1):(3 * k)] , "\n")
}
# if (max(abs(x-x_old)) < tol || abs(loss_new - loss_old) < loss_tol) {
#   break
# }
if (max(abs(x-x_old)) < tol ) {
break
}
gradient_old = gradient
x_old_2 = x_old
}
names(pi_frame) = paste0("pi_",1:k)
names(u_frame) = paste0("u_",1:k)
names(sigma_frame) = paste0("sigma_",1:k)
result_list = list(
loss = loss_list,
pi_value = (exp(x[1:k]) / sum(exp(x[1:k]))) %>% as.vector() ,
u = x[(k + 1):(2 * k)] %>% as.vector(),
sigma = x[(2 * k + 1):(3 * k)] %>% as.vector(),
iter = iter,
grad = grad_list,
u_frame = u_frame,
sigma_frame = sigma_frame,
pi_frame = pi_frame
)
return(result_list)
}
x = c(1,2,1,-1,0.33,0.2)
#nearly converged to true value n=100,n=500
#x=c(1,3,0,0,1,1)
#x=c(1,2,0,0,1,1)
#x=c(1.1,1.9,0.9,-0.8,1/2,1/3)
#x=c(1,1,1,1,1,1)
result_list = gradient_BB_descent(x,data_points,max_iter =1000,tol = 1e-10, loss_tol=1e-6,report = FALSE)
gradient_BB_descent = function(x,
data_points,
max_iter = 1000,
tol = 1e-5,
loss_tol=1e-5,
report = FALSE) {
iter = 0
x_old = rep(0, length(x))
k = length(x) / 3
loss_list = c()
grad_list = c()
u_frame = data.frame()
sigma_frame = data.frame()
pi_frame = data.frame()
loss = loss_function(x, data_points)
if (is.nan(loss) || is.infinite(loss)) {
stop("Initial loss is NaN or infinite")
}
loss_list = c(loss_list, loss)
gradient_old = rep(0, length(x))
x_old_2 = rep(0, length(x))
u_frame=rbind(x[(k+1):(2*k)],u_frame)
sigma_frame=rbind(x[(2*k+1):(3*k)],sigma_frame)
pi_frame=rbind(exp(x[1:k])/sum(exp(x[1:k])),pi_frame)
while (iter < max_iter) {
iter = iter + 1
x_old = x
gradient = grad_function(x, data_points)
if (any(is.nan(gradient)) || any(is.infinite(gradient))) {
print(as.vector(x_old))
names(pi_frame) = paste0("pi_",1:k)
names(u_frame) = paste0("u_",1:k)
names(sigma_frame) = paste0("sigma_",1:k)
result_list = list(
loss = loss_list,
pi_value = (exp(x[1:k]) / sum(exp(x[1:k]))) %>% as.vector() ,
u = x[(k + 1):2 * k] %>% as.vector(),
sigma = x[(2 * k + 1):(3 * k)] %>% as.vector(),
iter = iter,
grad = grad_list,
u_frame = u_frame,
sigma_frame = sigma_frame,
pi_frame = pi_frame
)
print(paste0("Gradient is NaN or infinite at iteration ", iter))
return(result_list)
#stop("Gradient is NaN or infinite at iteration ", iter)
#print(x_old)
}
step_size = as.vector((x_old - x_old_2) %*% (gradient - gradient_old) ) / sum((gradient - gradient_old) ^ 2)
#step_size = step_size * max(1,log10(1 + iter))
x = x - step_size * gradient
u_frame=rbind(x[(k+1):(2*k)],u_frame)
sigma_frame=rbind(x[(2*k+1):(3*k)],sigma_frame)
pi_frame=rbind(exp(x[1:k])/sum(exp(x[1:k])),pi_frame)
x[(2*k+1):(3*k)]=abs(x[(2*k+1):(3*k)])
loss_old = loss_new
loss_new = loss_function(x, data_points)
if (is.nan(loss_new) || is.infinite(loss_new)) {
names(pi_frame) = paste0("pi_",1:k)
names(u_frame) = paste0("u_",1:k)
names(sigma_frame) = paste0("sigma_",1:k)
result_list = list(
loss = loss_list,
pi_value = (exp(x[1:k]) / sum(exp(x[1:k]))) %>% as.vector() ,
u = x[(k + 1):(2 * k)] %>% as.vector(),
sigma = x[(2 * k + 1):(3 * k)] %>% as.vector(),
iter = iter,
grad = grad_list,
u_frame = u_frame,
sigma_frame = sigma_frame,
pi_frame = pi_frame
)
print(paste0("Loss is NaN or infinite at iteration ", iter))
return(result_list)
}
loss_list = c(loss_list, loss_new)
grad_list = c(grad_list, max(abs(gradient)))
if (report) {
cat("iter:", iter, "loss:", loss_new, "\n")
cat("pi:", exp(x[1:k]) / sum(exp(x[1:k])), "\n")
cat("u:", x[(k + 1):(2 * k)] , "\n")
cat("sigma:", x[(2 * k + 1):(3 * k)] , "\n")
}
# if (max(abs(x-x_old)) < tol || abs(loss_new - loss_old) < loss_tol) {
#   break
# }
if (max(abs(x-x_old)) < tol ) {
break
}
gradient_old = gradient
x_old_2 = x_old
}
names(pi_frame) = paste0("pi_",1:k)
names(u_frame) = paste0("u_",1:k)
names(sigma_frame) = paste0("sigma_",1:k)
result_list = list(
loss = loss_list,
pi_value = (exp(x[1:k]) / sum(exp(x[1:k]))) %>% as.vector() ,
u = x[(k + 1):(2 * k)] %>% as.vector(),
sigma = x[(2 * k + 1):(3 * k)] %>% as.vector(),
iter = iter,
grad = grad_list,
u_frame = u_frame,
sigma_frame = sigma_frame,
pi_frame = pi_frame
)
return(result_list)
}
x = c(1,2,1,-1,0.33,0.2)
#nearly converged to true value n=100,n=500
#x=c(1,3,0,0,1,1)
#x=c(1,2,0,0,1,1)
#x=c(1.1,1.9,0.9,-0.8,1/2,1/3)
#x=c(1,1,1,1,1,1)
result_list = gradient_BB_descent(x,data_points,max_iter =1000,tol = 1e-10, loss_tol=1e-6,report = FALSE)
gradient_BB_descent = function(x,
data_points,
max_iter = 1000,
tol = 1e-5,
loss_tol=1e-5,
report = FALSE) {
iter = 0
x_old = rep(0, length(x))
k = length(x) / 3
loss_list = c()
grad_list = c()
u_frame = data.frame()
sigma_frame = data.frame()
pi_frame = data.frame()
loss_new = loss_function(x, data_points)
if (is.nan(loss_new) || is.infinite(loss_new)) {
stop("Initial loss is NaN or infinite")
}
loss_list = c(loss_list, loss_new)
gradient_old = rep(0, length(x))
x_old_2 = rep(0, length(x))
u_frame=rbind(x[(k+1):(2*k)],u_frame)
sigma_frame=rbind(x[(2*k+1):(3*k)],sigma_frame)
pi_frame=rbind(exp(x[1:k])/sum(exp(x[1:k])),pi_frame)
while (iter < max_iter) {
iter = iter + 1
x_old = x
gradient = grad_function(x, data_points)
if (any(is.nan(gradient)) || any(is.infinite(gradient))) {
print(as.vector(x_old))
names(pi_frame) = paste0("pi_",1:k)
names(u_frame) = paste0("u_",1:k)
names(sigma_frame) = paste0("sigma_",1:k)
result_list = list(
loss = loss_list,
pi_value = (exp(x[1:k]) / sum(exp(x[1:k]))) %>% as.vector() ,
u = x[(k + 1):2 * k] %>% as.vector(),
sigma = x[(2 * k + 1):(3 * k)] %>% as.vector(),
iter = iter,
grad = grad_list,
u_frame = u_frame,
sigma_frame = sigma_frame,
pi_frame = pi_frame
)
print(paste0("Gradient is NaN or infinite at iteration ", iter))
return(result_list)
#stop("Gradient is NaN or infinite at iteration ", iter)
#print(x_old)
}
step_size = as.vector((x_old - x_old_2) %*% (gradient - gradient_old) ) / sum((gradient - gradient_old) ^ 2)
#step_size = step_size * max(1,log10(1 + iter))
x = x - step_size * gradient
u_frame=rbind(x[(k+1):(2*k)],u_frame)
sigma_frame=rbind(x[(2*k+1):(3*k)],sigma_frame)
pi_frame=rbind(exp(x[1:k])/sum(exp(x[1:k])),pi_frame)
x[(2*k+1):(3*k)]=abs(x[(2*k+1):(3*k)])
loss_old = loss_new
loss_new = loss_function(x, data_points)
if (is.nan(loss_new) || is.infinite(loss_new)) {
names(pi_frame) = paste0("pi_",1:k)
names(u_frame) = paste0("u_",1:k)
names(sigma_frame) = paste0("sigma_",1:k)
result_list = list(
loss = loss_list,
pi_value = (exp(x[1:k]) / sum(exp(x[1:k]))) %>% as.vector() ,
u = x[(k + 1):(2 * k)] %>% as.vector(),
sigma = x[(2 * k + 1):(3 * k)] %>% as.vector(),
iter = iter,
grad = grad_list,
u_frame = u_frame,
sigma_frame = sigma_frame,
pi_frame = pi_frame
)
print(paste0("Loss is NaN or infinite at iteration ", iter))
return(result_list)
}
loss_list = c(loss_list, loss_new)
grad_list = c(grad_list, max(abs(gradient)))
if (report) {
cat("iter:", iter, "loss:", loss_new, "\n")
cat("pi:", exp(x[1:k]) / sum(exp(x[1:k])), "\n")
cat("u:", x[(k + 1):(2 * k)] , "\n")
cat("sigma:", x[(2 * k + 1):(3 * k)] , "\n")
}
# if (max(abs(x-x_old)) < tol || abs(loss_new - loss_old) < loss_tol) {
#   break
# }
if (max(abs(x-x_old)) < tol ) {
break
}
gradient_old = gradient
x_old_2 = x_old
}
names(pi_frame) = paste0("pi_",1:k)
names(u_frame) = paste0("u_",1:k)
names(sigma_frame) = paste0("sigma_",1:k)
result_list = list(
loss = loss_list,
pi_value = (exp(x[1:k]) / sum(exp(x[1:k]))) %>% as.vector() ,
u = x[(k + 1):(2 * k)] %>% as.vector(),
sigma = x[(2 * k + 1):(3 * k)] %>% as.vector(),
iter = iter,
grad = grad_list,
u_frame = u_frame,
sigma_frame = sigma_frame,
pi_frame = pi_frame
)
return(result_list)
}
x = c(1,2,1,-1,0.33,0.2)
#nearly converged to true value n=100,n=500
#x=c(1,3,0,0,1,1)
#x=c(1,2,0,0,1,1)
#x=c(1.1,1.9,0.9,-0.8,1/2,1/3)
#x=c(1,1,1,1,1,1)
result_list = gradient_BB_descent(x,data_points,max_iter =1000,tol = 1e-10, loss_tol=1e-6,report = FALSE)
result_list[2:5]
result_list$loss[5:result_list$iter] %>% plot( ,type="l",main="loss")
result_list$grad %>% plot( ,type="l",main="absolute norm of grad")
result_list$u_frame%>% plot(,main="u")
result_list$sigma_frame%>% plot(,main="sigma")
result_list$pi_frame%>% plot(,main="pi",ylim=c(0,1))
x = c(1,2,1,-1,0.33,0.2)
#nearly converged to true value n=100,n=500
#x=c(1,3,0,0,1,1)
#x=c(1,2,0,0,1,1)
#x=c(1.1,1.9,0.9,-0.8,1/2,1/3)
#x=c(1,1,1,1,1,1)
result_list = gradient_BB_descent(x,data_points,max_iter =1000,tol = 1e-5, loss_tol=1e-6,report = FALSE)
result_list[2:5]
result_list$loss[5:result_list$iter] %>% plot( ,type="l",main="loss")
result_list$grad %>% plot( ,type="l",main="absolute norm of grad")
result_list$u_frame%>% plot(,main="u")
result_list$sigma_frame%>% plot(,main="sigma")
result_list$pi_frame%>% plot(,main="pi",ylim=c(0,1))
#x = c(1,2,1,-1,0.33,0.2) # converged to true value within 40 steps n=100
x=c(1,3,0,0,1,1)
#x=c(1,2,0,0,1,1)
#x=c(1.1,1.9,0.9,-0.8,1/2,1/3)
#x=c(1,1,1,1,1,1)
result_list = gradient_BB_descent(x,data_points,max_iter =1000,tol = 1e-5, loss_tol=1e-6,report = FALSE)
result_list[2:5]
result_list$loss[5:result_list$iter] %>% plot( ,type="l",main="loss")
result_list$grad %>% plot( ,type="l",main="absolute norm of grad")
result_list$u_frame%>% plot(,main="u")
result_list$sigma_frame%>% plot(,main="sigma")
result_list$pi_frame%>% plot(,main="pi",ylim=c(0,1))
#x = c(1,2,1,-1,0.33,0.2) # converged to true value within 40 steps n=100
#x=c(1,3,0,0,1,1) # not converged to true value
x=c(1,2,0,0,1,1)
#x=c(1.1,1.9,0.9,-0.8,1/2,1/3)
#x=c(1,1,1,1,1,1)
result_list = gradient_BB_descent(x,data_points,max_iter =1000,tol = 1e-5, loss_tol=1e-6,report = FALSE)
result_list[2:5]
result_list$loss[5:result_list$iter] %>% plot( ,type="l",main="loss")
result_list$grad %>% plot( ,type="l",main="absolute norm of grad")
result_list$u_frame%>% plot(,main="u")
result_list$sigma_frame%>% plot(,main="sigma")
result_list$pi_frame%>% plot(,main="pi",ylim=c(0,1))
#x = c(1,2,1,-1,0.33,0.2) # converged to true value within 40 steps n=100
#x=c(1,3,0,0,1,1) # not converged to true value
#x=c(1,2,0,0,1,1) # not converged to true value
x=c(1.1,1.9,0.9,-0.8,1/2,1/3)
#x=c(1,1,1,1,1,1)
result_list = gradient_BB_descent(x,data_points,max_iter =1000,tol = 1e-5, loss_tol=1e-6,report = FALSE)
result_list[2:5]
result_list$loss[5:result_list$iter] %>% plot( ,type="l",main="loss")
result_list$grad %>% plot( ,type="l",main="absolute norm of grad")
result_list$u_frame%>% plot(,main="u")
result_list$sigma_frame%>% plot(,main="sigma")
result_list$pi_frame%>% plot(,main="pi",ylim=c(0,1))
#x = c(1,2,1,-1,0.33,0.2) # converged to true value within 40 steps n=100
#x=c(1,3,0,0,1,1) # not converged to true value
#x=c(1,2,0,0,1,1) # not converged to true value
#x=c(1.1,1.9,0.9,-0.8,1/2,1/3) # not converged to true value
x=c(1,1,1,1,1,1)
result_list = gradient_BB_descent(x,data_points,max_iter =1000,tol = 1e-5, loss_tol=1e-6,report = FALSE)
result_list[2:5]
result_list$loss[5:result_list$iter] %>% plot( ,type="l",main="loss")
result_list$grad %>% plot( ,type="l",main="absolute norm of grad")
result_list$u_frame%>% plot(,main="u")
result_list$sigma_frame%>% plot(,main="sigma")
result_list$pi_frame%>% plot(,main="pi",ylim=c(0,1))
#x = c(1,2,1,-1,0.33,0.2) # converged to true value within 40 steps n=100
x = c(5,4,0,0,2,1)
#x=c(1,3,0,0,1,1) # not converged to true value
#x=c(1,2,0,0,1,1) # not converged to true value
#x=c(1.1,1.9,0.9,-0.8,1/2,1/3) # not converged to true value
#x=c(1,1,1,1,1,1) not congerged to true value
result_list = gradient_BB_descent(x,data_points,max_iter =1000,tol = 1e-5, loss_tol=1e-6,report = FALSE)
result_list[2:5]
result_list$loss[5:result_list$iter] %>% plot( ,type="l",main="loss")
result_list$grad %>% plot( ,type="l",main="absolute norm of grad")
result_list$u_frame%>% plot(,main="u")
result_list$sigma_frame%>% plot(,main="sigma")
result_list$pi_frame%>% plot(,main="pi",ylim=c(0,1))
#x = c(1,2,1,-1,0.33,0.2) # converged to true value within 40 steps n=100
x = c(5,4,0,0,2,1)
#x=c(1,3,0,0,1,1) # not converged to true value
#x=c(1,2,0,0,1,1) # not converged to true value
#x=c(1.1,1.9,0.9,-0.8,1/2,1/3) # not converged to true value
#x=c(1,1,1,1,1,1) not congerged to true value
result_list = gradient_BB_descent(x,data_points,max_iter =1000,tol = 1e-6, loss_tol=1e-6,report = FALSE)
result_list[2:5]
result_list$loss[5:result_list$iter] %>% plot( ,type="l",main="loss")
result_list$grad %>% plot( ,type="l",main="absolute norm of grad")
result_list$u_frame%>% plot(,main="u")
result_list$sigma_frame%>% plot(,main="sigma")
result_list$pi_frame%>% plot(,main="pi",ylim=c(0,1))
library(dplyr)
set.seed(123)
u1 = 1
sigma1 = 1/3
u2=-1
sigma2=1/5
pi_1 = 1/3
pi_2 = 2/3
n=500
uniform_random = runif(n,0,1)
data_points = (uniform_random<=pi_1) * rnorm(n, u1, sigma1) + (uniform_random>pi_1) * rnorm(n, u2, sigma2)
#x = c(1,2,1,-1,0.33,0.2) #True value, converged to true value within 40 steps n=100
x = c(5,4,0,0,2,1) # not converged to true value
#x=c(1,3,0,0,1,1) # not converged to true value
#x=c(1,2,0,0,1,1) # not converged to true value
#x=c(1.1,1.9,0.9,-0.8,1/2,1/3) # not converged to true value
#x=c(1,1,1,1,1,1) not congerged to true value
result_list = gradient_BB_descent(x,data_points,max_iter =1000,tol = 1e-5, loss_tol=1e-6,report = FALSE)
result_list[2:5]
result_list$loss[5:result_list$iter] %>% plot( ,type="l",main="loss")
result_list$grad %>% plot( ,type="l",main="absolute norm of grad")
result_list$u_frame%>% plot(,main="u")
result_list$sigma_frame%>% plot(,main="sigma")
result_list$pi_frame%>% plot(,main="pi",ylim=c(0,1))
